{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2.\n",
    "\n",
    "## Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialdata2021/wiki/Assignment-1-and-2) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Monday April 5th, 2021 at 23:55. Hand in your files via [`http://peergrade.io`](http://peergrade.io/).\n",
    "\n",
    "**Peergrading date and time**: _Remember that after handing in you have a week to evaluate a few assignments written by other members of the class_. Thus, the peer evaluations are due on Monday April 12th, 2021 at 23:55. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Questions to text and lectures.\n",
    "\n",
    "A) Please answer my questions to the Segal and Heer paper we read during lecture 7 and 8.\n",
    "\n",
    "* What is the *Oxford English Dictionary's* defintion of a narrative?\n",
    "* What is your favorite visualization among the examples in section 3? Explain why in a few words.\n",
    "* What's the point of Figure 7?\n",
    "* Use Figure 7 to find the most common design choice within each category for the Visual narrative and Narrative structure (the categories within visual narrative are 'visual structuring', 'highlighting', etc).\n",
    "* Check out Figure 8 and section 4.3. What is your favorite genre of narrative visualization? Why? What is your least favorite genre? Why?\n",
    "\n",
    "\n",
    "B) Also please answer the questions to my talk on [explanatory data visualization](https://www.youtube.com/watch?v=yHKYMGwefso)\n",
    "\n",
    "* What are the three key elements to keep in mind when you design an explanatory visualization?\n",
    "* In the video I talk about (1) *overview first*,  (2) *zoom and filter*,  (3) *details on demand*. \n",
    "  - Go online and find a visualization that follows these principles (don't use one from the video). \n",
    "  - Explain how it does achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.\n",
    "* Explain in your own words: How is explanatory data analysis different from exploratory data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random forest and weather\n",
    "\n",
    "The aim here is to recreate the work you did in Part 1-3 of the Week 7 lecture. I've phrased things differently relative to the exercise to make the purpose more clear. \n",
    "\n",
    "Part 2A: Random forest binary classification. \n",
    "* Using the and instructions and material from Week 7, build a *random forest* classifier to distinguish between two types (you choose) of crime using on spatio-temporal (where/when) features of data describing the two crimes. When you're done, you should be able to give the classifier a place and a time, and it should tell you which of the two  types of crime happened there.\n",
    "  - Explain about your choices for training/test data, features, and encoding. (You decide how to present your results, but here are some example topics to consider: Did you balance the training data? What are the pros/cons of balancing? Do you think your model is overfitting? Did you choose to do cross-validation? Which specific features did you end up using? Why? Which features (if any) did you one-hot encode? Why ... or why not?))\n",
    "  - Report accuracy. Discuss the model performance.\n",
    "  \n",
    "  \n",
    "Part 2B: Info from weather features.\n",
    "* Now add features from weather data to your random forest. \n",
    "  - Report accuracy. \n",
    "  - Discuss how the model performance changes relative to the version with no weather data.\n",
    "  - Discuss what you have learned about crime from including weather data in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r\"C:\\Users\\andri\\Documents\\Andri Geir\\DTU\\Social Data\\Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")\n",
    "\n",
    "df= df[(pd.to_datetime(df['Date']) >= '01/01/2003')] \n",
    "df = df[(pd.to_datetime(df['Date']) <= '31/12/2017')] \n",
    "df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['Date']).day\n",
    "df['Hour'] = [int(time[0:2]) for time in (df['Time'])]\n",
    "\n",
    "## Creating balanced dataset\n",
    "df_filtered1 = df[df[\"Category\"]==(\"VEHICLE THEFT\")].sample(40000)\n",
    "df_filtered2 = df[df[\"Category\"]==(\"FRAUD\")].sample(40000)\n",
    "df_balanced = pd.concat([df_filtered1,df_filtered2]).sample(frac=1).reset_index()\n",
    "\n",
    "# One hot Encoding and factorizing the categorical target variable.\n",
    "X = pd.get_dummies(df_balanced[[\"PdDistrict\",\"Year\",\"Month\",\"Day\",\"Hour\"]])\n",
    "y = df_balanced[\"Category\"]\n",
    "y = y.factorize( ['VEHICLE THEFT', 'FRAUD'] )[0]\n",
    "\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and test/train split\n",
    "To begin with the data is loaded, filtered for dates from 2003-2018 and then \"Month of the year\" and \"Time of the day\" columns added according to instructions. The we draw equal amounts of observations from each category in order to maintain balance in the data set Having such a balanced data set ensures that the model is gets euqal amount of each class but it also \"deletes\" the information that one category is more common than the other. This information could of course greatly improve such a model in cases where the model maybe isn't very . The dataset is also shuffled so that train and test draw later isn't biast.\n",
    "\n",
    "Before we split into the train and test the categorical varialbe \"PdDistrict\" is one hot encoded because the model can not understand text so that variable is split into multible and represented as binary one for each district. The target variable which is categorical with only two classes is factorized to be either 0 or 1, again so the model can understand it.\n",
    "\n",
    "Finally the data set is split into 75% train data and 25% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "error  = abs(test_labels-predictions.round()).sum()\n",
    "(len(test_labels)-error)/len(test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy \n",
    "The model predicts 68% correct which is better than guessing but still not very impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4855])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Prediction for one observation\n",
    "rf.predict(np.array(test_features.iloc[0,:]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one want to make a prediction for a new observation the above code is an example of this. After rounding this will indictate  1 or 0 for either predicting the crime to be a Vehicle Theft or Fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6688575024472102"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REading dat and localizing the time after instructions on slack.\n",
    "weather = pd.read_csv(\"weather_data.csv\", parse_dates=[\"date\"],\n",
    "                date_parser=lambda x: pd.to_datetime(x).tz_convert(None).tz_localize(\"Etc/GMT+3\").tz_convert(\"Etc/GMT-7\")) \n",
    "# parse_dates specifies what columns contain dates (instead of a string column -> it becomes a date_time column)\n",
    "# data_parser -> we specify our custom date_parser (Pandas has default data_parser, usually we do not need to specify it)\n",
    "# in our data_parser we use \"lambda\" function - it means that we want to apply something to each value in the column\n",
    "# pd.to_datetime(x) - converts each value to date_time obect. By default pd.to_datetime assigns GMT0 timezone, \n",
    "# which is wrong, thus, we specification of timezone with tz_convert(None)\n",
    "# now we want to specify the correct timezone -> we use tz_localize(\"..\")\n",
    "# after we can convert dates to the actual SanFrancisco timezone with tz_convert(\"..\")\n",
    "# weather.head()\n",
    "\n",
    "\n",
    "#Loading and adding relevant columns again\n",
    "df = pd.read_csv(r\"C:\\Users\\andri\\Documents\\Andri Geir\\DTU\\Social Data\\Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\", usecols=[\"Category\", \"Date\", \"Time\", \"PdDistrict\"]) ## specify any columns you need\n",
    "\n",
    "df = df[df[\"Category\"].isin(['VEHICLE THEFT', 'FRAUD'])] # filter out the dataframe, you can plug any list of crimes\n",
    "df[\"datetime\"] = df.apply(lambda x: pd.to_datetime(x.Date + \" \" + x.Time).round(\"H\").tz_localize(\"ETC/GMT-7\"), axis = 1)  \n",
    "df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['Date']).day\n",
    "df['Hour'] = [int(time[0:2]) for time in (df['Time'])]\n",
    "# Here we do a bit more complicated thing\n",
    "# .apply allows us to use function for each row of a dataframe (read documentation for more info)\n",
    "# so we take a row (which is x) and take cell of Date and Time -> and concatenate them to one big string\n",
    "# that can be then converted to datetime. We would also want to remove any seconds and minutes (round to hours)\n",
    "# then we specify that dates are in GMT-7\n",
    "# the result is going to be stored in new \"datetime\" column\n",
    "\n",
    "\n",
    "# Left joining in by time stamp in order to get the relevant weather data for each observation.\n",
    "df_weather = pd.merge(df,weather,how=\"left\",left_on =\"datetime\",right_on =\"date\")\n",
    "df_weather = df_weather.dropna(subset=[\"date\"])\n",
    "\n",
    "#Balancing the data after the observatoions with the wather data missing have been dropped.\n",
    "df_filtered1 = df_weather[df_weather[\"Category\"]==(\"VEHICLE THEFT\")].sample(14301)\n",
    "df_filtered2 = df_weather[df_weather[\"Category\"]==(\"FRAUD\")].sample(14301)\n",
    "\n",
    "#Constructing the final df to work with.\n",
    "df_balanced = pd.concat([df_filtered1,df_filtered2]).sample(frac=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "X = pd.get_dummies(df_balanced[['PdDistrict', 'Year',\n",
    "       'Month', 'Day', 'Hour', 'temperature', 'humidity', 'weather',\n",
    "       'wind_speed', 'wind_direction', 'pressure']])\n",
    "\n",
    "y = df_balanced[\"Category\"]\n",
    "y = y.factorize( ['VEHICLE THEFT', 'FRAUD'] )[0]\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "#print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "error  = abs(test_labels-predictions.round()).sum()\n",
    "(len(test_labels)-error)/len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is exactly like before and the same variables are one hot encoded and factorized. All of the weather data is continous so no need to manipulate that. \n",
    "\n",
    "The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data visualization\n",
    "\n",
    "* Create the Bokeh visualization from Part 2 of the Week 8 Lecture, displayed in a beautiful `.gif` below. \n",
    "* Provide nice comments for your code. Don't just use the `# inline comments`, but the full Notebook markdown capabilities and explain what you're doing.\n",
    "\n",
    "![Movie](https://github.com/suneman/socialdataanalysis2020/blob/master/files/week8_1.gif?raw=true \"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
